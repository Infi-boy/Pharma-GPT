{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"13IxFDkc-gwQLwnIQ8mS1-avzQBOyHdWt","authorship_tag":"ABX9TyMU7CP4lSiv0YJTh3pyoZlR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"F59KaW6IVZnK"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Embedding, LayerNormalization, Dropout\n","from tensorflow.keras.models import Model\n","import numpy as np\n","\n","# Scaled Dot-Product Attention called in below class of Multihead Attention\n","def scaled_dot_product_attention(query, key, value, mask):\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","    scale = tf.math.sqrt(tf.cast(tf.shape(key)[-1], tf.float32))\n","    logits = matmul_qk / scale\n","\n","    if mask is not None:\n","        logits += (mask * -1e9)\n","\n","    attention_weights = tf.nn.softmax(logits, axis=-1)\n","    output = tf.matmul(attention_weights, value)\n","    return output, attention_weights\n","\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads):\n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","        self.depth = d_model // num_heads\n","\n","        self.wq = Dense(d_model)\n","        self.wk = Dense(d_model)\n","        self.wv = Dense(d_model)\n","        self.dense = Dense(d_model)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","    def call(self, query, key, value, mask):\n","        batch_size = tf.shape(query)[0]\n","\n","        query = self.wq(query)\n","        key = self.wk(key)\n","        value = self.wv(value)\n","\n","        query = self.split_heads(query, batch_size)\n","        key = self.split_heads(key, batch_size)\n","        value = self.split_heads(value, batch_size)\n","\n","        output, attention_weights = scaled_dot_product_attention(query, key, value, mask)\n","        output = tf.transpose(output, perm=[0, 2, 1, 3])\n","        output = tf.reshape(output, (batch_size, -1, self.d_model))\n","\n","        return self.dense(output), attention_weights\n","\n","def scaled_dot_product_attention(query, key, value, mask):\n","    matmul_qk = tf.matmul(query, key, transpose_b=True)\n","\n","    dk = tf.cast(tf.shape(key)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    if mask is not None:\n","        scaled_attention_logits += (mask * -1e9)\n","\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","    output = tf.matmul(attention_weights, value)\n","\n","    return output, attention_weights\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    return tf.keras.Sequential([\n","        Dense(dff, activation='relu'),\n","        Dense(d_model)\n","    ])\n","\n","\n","class DecoderBlock(tf.keras.layers.Layer):\n","    def __init__(self, d_model, num_heads, dff, rate=0.1, name=None, **kwargs):\n","        super(DecoderBlock, self).__init__(name=name, **kwargs)\n","        self.mha1 = MultiHeadAttention(d_model, num_heads)\n","        self.mha2 = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n","        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = Dropout(rate)\n","        self.dropout2 = Dropout(rate)\n","        self.dropout3 = Dropout(rate)\n","\n","    @tf.function\n","    def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n","        attn1, _ = self.mha1(x, x, x, look_ahead_mask)\n","        attn1 = self.dropout1(attn1, training=training)\n","        out1 = self.layernorm1(x + attn1)\n","\n","        # attn2, _ = self.mha2(out1, enc_output, enc_output, padding_mask)\n","        # attn2 = self.dropout2(attn2, training=training)\n","        # out2 = self.layernorm2(out1 + attn2)\n","\n","        ffn_output = self.ffn(out1)\n","        # ffn_output = self.ffn(out2)\n","        ffn_output = self.dropout3(ffn_output, training=training)\n","        return self.layernorm3(out1 + ffn_output)\n","\n","    def get_config(self):\n","        config = super(DecoderBlock, self).get_config()\n","        config.update({\n","            'd_model': self.mha1.d_model,\n","            'num_heads': self.mha1.num_heads,\n","            'dff': self.ffn.layers[0].units,\n","            'rate': self.dropout1.rate,\n","        })\n","        return config\n","\n","def create_look_ahead_mask(size):\n","    mask = tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    mask = tf.cast(mask, tf.float32)\n","    return mask\n","\n","class GPT(tf.keras.Model):\n","    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, maximum_position_encoding, rate=0.1, name=None, **kwargs):\n","        super(GPT, self).__init__(name=name, **kwargs)\n","        self.num_layers = num_layers\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.dff = dff\n","        self.vocab_size = vocab_size\n","        self.maximum_position_encoding = maximum_position_encoding\n","        self.rate = rate\n","\n","        self.embedding = Embedding(vocab_size, d_model)\n","        self.position_encoding = self.positional_encoding(maximum_position_encoding, d_model)\n","        self.decoder_blocks = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","        self.dropout = Dropout(rate)\n","        # self.final_layer = Dense(vocab_size)\n","        self.final_layer = Dense(vocab_size)\n","\n","    def positional_encoding(self, position, d_model):\n","        angle_rads = self.get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n","        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","        return tf.cast(angle_rads[np.newaxis, ...], tf.float32)\n","\n","    def get_angles(self, position, i, d_model):\n","        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n","        return position * angle_rates\n","\n","    def call(self, x, training):\n","        seq_len = tf.shape(x)[1]\n","        x = self.embedding(x)\n","        x += self.position_encoding[:, :seq_len, :]\n","        x = self.dropout(x, training=training)\n","\n","        look_ahead_mask = create_look_ahead_mask(seq_len)\n","        for block in self.decoder_blocks:\n","            x = block(x, x, look_ahead_mask, None, training=training)\n","\n","        return self.final_layer(x)\n","\n","    def get_config(self):\n","        config = super(GPT, self).get_config()\n","        config.update({\n","            'num_layers': self.num_layers,\n","            'd_model': self.d_model,\n","            'num_heads': self.num_heads,\n","            'dff': self.dff,\n","            'vocab_size': self.vocab_size,\n","            'maximum_position_encoding': self.maximum_position_encoding,\n","            'rate': self.rate\n","        })\n","        return config\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        return cls(**config)\n","\n"]},{"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","tf.config.run_functions_eagerly(True)\n","df = pd.read_csv('Data/ai_data.csv', index_col = [0])\n","df = df[['seq', 'SMILE', 'InChI']]\n","df.dropna(inplace = True)\n","len(df)\n","\n","tmp = pd.DataFrame(df[['seq', 'SMILE']].agg(' '.join, axis=1).str.len()).rename(columns = {0:'length'})\n","df = df[tmp.length < 4097].copy()\n","#####\n","\n","seq_map = pd.read_csv('Data/seq_map.csv', index_col = [0])\n","smile_map = pd.read_csv('Data/smile_map.csv', index_col = [0])\n","inchi_map = pd.read_csv('Data/inchi_map.csv', index_col = [0])\n"],"metadata":{"id":"yiiBcwM6WC8K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.iloc[0].SMILE"],"metadata":{"id":"HNxziSC-n843"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example Usage\n","# Example Usage\n","vocabulary = list(set(['[START]', '[END]', '[SEP]'] + list(set ( list(seq_map.seq) + list(smile_map.smile)))))\n","\n","vocab_size = len(vocabulary)\n","d_model = 512\n","num_heads = 8\n","dff = 61\n","num_layers = 1\n","# maximum_position_encoding = 12000\n","maximum_position_encoding = 4097\n","\n","dropout_rate = 0.1\n","\n","model = GPT(num_layers, d_model, num_heads, dff, vocab_size, maximum_position_encoding, dropout_rate)\n","\n","# Dummy data for testing\n","input_sequence = tf.constant([[1, 2, 3, 9], [5, 6, 7, 8]])\n","\n","# Forward pass\n","output = model(input_sequence, training=False)"],"metadata":{"id":"xunbrqvcV7qR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"6WIKm28E_tux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","# max_length = 4096\n","class CustomTokenizer:\n","    def __init__(self, vocabulary):\n","        # Create vocabulary mappings\n","        self.vocab = {word: idx for idx, word in enumerate(vocabulary)}\n","        self.idx_to_word = {idx: word for word, idx in self.vocab.items()}\n","\n","        # Handle special tokens\n","        self.start_token_id = self.vocab.get('[START]')\n","        self.end_token_id = self.vocab.get('[END]')\n","        self.sep_token_id = self.vocab.get('[SEP]')\n","        self.vocab_size = len(vocabulary)\n","\n","    def encode(self, input_text):\n","        # Tokenize input and output text, adding separator\n","        input_tokens = input_text.split()\n","\n","\n","        input_ids = [self.vocab.get(token, self.vocab.get('[END]')) for token in input_tokens]\n","        # Combine with start and end tokens\n","        return [self.start_token_id] + input_ids\n","\n","    def encode_test(self, input_text):\n","        # Tokenize input and output text, adding separator\n","        input_tokens = input_text.split('[SEP]')\n","        input_tokens = list(input_tokens[0]) + ['[SEP]'] + list(input_tokens[1])\n","        input_ids = [self.vocab.get(token, self.vocab.get('[END]')) for token in input_tokens]\n","        # Combine with start and end tokens\n","        return [self.start_token_id] + input_ids\n","    def decode(self, token_ids):\n","        # Convert token IDs back to text, excluding special tokens\n","        tokens = [self.idx_to_word.get(token_id) for token_id in token_ids if token_id not in [self.start_token_id, self.end_token_id, self.sep_token_id]]\n","        return ' '.join(tokens)\n","    def vocab_output_vector(self, token):\n","        # Initialize vector of zeros with size equal to vocab size\n","        output_vector = np.zeros(self.vocab_size)\n","        # Get the index of the token and set the corresponding index to 1\n","        token_id = self.vocab.get(token, self.vocab.get('[END]'))  # Default to [END] if token not found\n","        output_vector[token_id] = 1\n","        return output_vector\n","\n","\n","vocabulary = list(set(['[START]', '[END]', '[SEP]'] + list(set ( list(seq_map.seq) + list(smile_map.smile)))))\n","tokenizer = CustomTokenizer(vocabulary)"],"metadata":{"id":"zCOw6dwMXdzj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def out(inp):\n","  out = []\n","  if '[END]' in inp:\n","    inp = inp.split('[END]')[0]\n","    for j in list (inp):\n","      out.append(tokenizer.vocab_output_vector(j))\n","    out.append(tokenizer.vocab_output_vector('[END]'))\n","    out = np.array(out)\n","    return np.expand_dims(out, axis = 0)\n","  if '[SEP]' in inp:\n","    li = list(inp.split('[SEP]')[0]) + ['[SEP]'] + list(inp.split('[SEP]')[1])\n","  else:\n","    li = list(inp)\n","  for j in li:\n","    out.append(tokenizer.vocab_output_vector(j))\n","  out = np.array(out)\n","  return np.expand_dims(out, axis = 0)\n","\n","def out_sparse(inp):\n","  out = []\n","\n","  if '[END]' in inp:\n","      inp = inp.split('[END]')[0]  # Get input before [END]\n","\n","      # Append token IDs instead of one-hot vectors\n","      for j in list(inp):\n","          out.append(tokenizer.vocab.get(j, tokenizer.end_token_id))  # Use vocab mapping\n","\n","      out.append(tokenizer.end_token_id)  # Append [END] token ID\n","      return np.expand_dims(np.array(out), axis=0)  # Shape: (1, seq_len)\n","\n","  if '[SEP]' in inp:\n","      # Handle [SEP] token correctly\n","      li = list(inp.split('[SEP]')[0]) + [tokenizer.sep_token_id] + list(inp.split('[SEP]')[1])\n","  else:\n","      li = list(inp)\n","\n","  # Append token IDs instead of one-hot vectors\n","  for j in li:\n","      out.append(tokenizer.vocab.get(j, tokenizer.end_token_id))  # Use vocab mapping\n","\n","  return np.expand_dims(np.array(out), axis=0)  # Shape: (1, seq_len\n","\n","def pad_outputs(outputs):\n","    # Find the maximum length of the second dimension from the outputs\n","    max_middle_dim = max(arr.shape[1] for arr in outputs)\n","\n","    padded_arr_list = []\n","    for arr in outputs:\n","        # Calculate the amount of padding needed for the second dimension\n","        padding_size = max_middle_dim - arr.shape[1]\n","\n","        # Pad the second dimension with zeros\n","        padded_arr = np.pad(arr, ((0, 0), (0, padding_size)), mode='constant')\n","        padded_arr_list.append(padded_arr)\n","\n","    # Concatenate all the padded arrays along the first dimension (batch dimension)\n","    output_data = np.concatenate(padded_arr_list, axis=0)\n","    return output_data\n","\n","loss_fun = 2\n","\n","batch = 221170*2\n","input_data = []\n","output_data = []\n","frac = len(df) / batch\n","for i in range(0,int(batch)):\n","  print(len(df) / batch)\n","  print(i*frac, (i + 1)* frac)\n","  tmp = df[int(i*frac) : int((i + 1)* frac )].copy()\n","  for i, row in tmp.iterrows():\n","    inputs = []\n","    outputs = []\n","    inp = row.seq + '[SEP]'\n","    inputs.append(inp)\n","\n","    inp = inp + row.SMILE[0]\n","    if loss_fun == 1:\n","      # Categorical cross entropy\n","      outputs.append( out(inp[1:]) )\n","\n","      for j in range(1,len(row.SMILE)):\n","        inputs.append(inp)\n","        inp = inp + str(row.SMILE[j])\n","        outputs.append( out(inp[1:]) )\n","      inputs.append(inp)\n","      outputs.append(out(inp[1:] + '[END]'))\n","      model.compile(\n","          # optimizer=tf.keras.optimizers.Adam(),\n","          optimizer='adam',\n","          loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","          metrics=['accuracy']\n","      )\n","      padded_arr_list = []\n","      max_middle_dim = max(arr.shape[1] for arr in outputs)\n","      for arr in outputs:\n","          padding_size = max_middle_dim - arr.shape[1]\n","          # Pad the second dimension with zeros\n","          padded_arr = np.pad(arr, ((0, 0), (0, padding_size), (0, 0)), mode='constant')\n","          padded_arr_list.append(padded_arr)\n","      output_data = np.concatenate(padded_arr_list, axis=0)\n","    if loss_fun == 2:\n","    # sparse categorical cross entropy\n","      outputs.append( out_sparse(inp[1:]) )\n","\n","      for j in range(1,len(row.SMILE)):\n","        inputs.append(inp)\n","        inp = inp + str(row.SMILE[j])\n","        outputs.append( out_sparse(inp[1:]) )\n","      inputs.append(inp)\n","      outputs.append(out_sparse(inp[1:] + '[END]'))\n","      output_data = pad_outputs(outputs)\n","\n","      model.compile(\n","          optimizer='adam',  # Choose an appropriate optimizer\n","          loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Use from_logits=True if outputs are logits\n","          metrics=['accuracy']  # Optional: you can add other metrics as needed\n","      )\n","\n","\n","    max_middle_dim = max(arr.shape[1] for arr in outputs)\n","\n","    # loss function :categorical cross entropy\n","\n","    # padded_arr_list = []\n","    # for arr in outputs:\n","    #     padding_size = max_middle_dim - arr.shape[1]\n","    #     # Pad the second dimension with zeros\n","    #     padded_arr = np.pad(arr, ((0, 0), (0, padding_size), (0, 0)), mode='constant')\n","    #     padded_arr_list.append(padded_arr)\n","    # output_data = np.concatenate(padded_arr_list, axis=0)\n","\n","    # #sparse categorical cross entropy\n","    # output_data = pad_outputs(outputs)\n","\n","    for j in range(len(inputs)):\n","      inputs[j] = tokenizer.encode_test(inputs[j])\n","\n","    max_length = max_middle_dim\n","    padded_array = np.array([np.pad(sublist, (0, max_length - len(sublist)), mode='constant', constant_values=0)\n","                            for sublist in inputs])\n","    input_data = padded_array\n","  break"],"metadata":{"id":"7XQovO_OXjj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_data.shape, output_data.shape"],"metadata":{"id":"giFoUcuBZxUp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(\n","    input_data,     # Padded input sequences\n","    output_data,    # One-hot encoded target tokens\n","    batch_size=1,  # Adjust based on your memory and dataset size\n","    epochs=10\n",")"],"metadata":{"id":"Bhm3WlRGXykJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length = 150\n","def infer (prompt):\n","  for i in range(max_length):\n","    seq = tokenizer.encode_test(prompt)\n","    seq_inp = np.expand_dims(np.array(seq), axis = 0)\n","    pred = model(seq_inp, training = False)\n","    last_token_logits = pred[:, -1, :]\n","    predicted_id = tf.argmax(last_token_logits, axis=-1).numpy()\n","    pred_char = tokenizer.decode([predicted_id[0]])\n","    if pred_char == '[END]':\n","      prompt += '.'\n","      return\n","    prompt += pred_char\n","  return prompt\n","\n","result = infer (tmp.seq.iloc[0] + '[SEP]')\n","result"],"metadata":{"id":"1LfdsdYsX3Ix"},"execution_count":null,"outputs":[]}]}